

# Set up

Import packages
```{r}
suppressPackageStartupMessages({
  suppressWarnings({
  library(tidyverse)   # Data wrangling
  library(here)        # File management
  library(readxl)      # Read excel files
  library(igraph)      # Network analysis
  })})
```

Set settings and universal variables
```{r setup, include=FALSE}
knitr::opts_knit$set(root.dir = here())
```

# Import data

Cleaned & deduplicated network data
```{r}
network = read_excel("data/Source/Network data/Final data 10.27.25 latlon comp.xlsx", na = "NA")
```


# Cleaning and processing

- Remove columns we definitely don't need
- Rename columns and reorder remaining columns
- Convert any columns with 0/1 to No/Yes, or in the case of gw, combine both columns into one categorical column
- Add new column for system size based on population served (thresholds provided by Kristin)
- Clean up errors in source type column
- Standardise all source lat/longs (there are currently different values for the same source)

```{r}
network_master = network%>%
  rename(pop_served = Population_served_count, source_name = Source)%>%
  rename_with(tolower)%>%
  
  mutate(
    purchased = case_when(
      purchased == "Yes" ~ 1,
      purchased == "No" ~ 0,
      T ~ NA),
    pop_served = ifelse(pop_served <= 1, NA, pop_served),
    system_size = case_when(
      is.na(pop_served) ~ NA,
      pop_served > 100000 ~ "Very Large",
      pop_served > 10000 ~ "Large",
      pop_served > 3300 ~ "Medium",
      pop_served > 500 ~ "Small",
      pop_served <= 500 ~ "Very Small"),
    source_type = case_when(
      source_name == "Kern River" ~ "River",
      source_name == "Lake Havasu" ~ "Lake",
      source_name == "Lake Spaulding" ~ "Lake",
      T ~ source_type))%>%
  
  group_by(source_name)%>%
  mutate(num_systems_served = n())%>%
  ungroup()%>%
  
  group_by(pwsid)%>%
  mutate(num_sw_sources = n())%>%
  ungroup()%>%

  select(-c(system_lat:system_lon, source_lat:source_lon, is_wholesaler, source_number, source_summary, combined_source_notes, combined_calsim_notes, swp:colorado_incompleteandnotedited, x2020_ccr_percent:x2023_percent, summer_2025_cleaning, calsim_demandunit:calsim_notes_sept2025, colorado_loop, gw_access_no_cc:gw_compliant_w_dw_standards_no_cc, connection_id, source_id))%>%
  
  select(1:3,5,4,13,15, 6:7,14, 8:12)%>%
  rename(swp = swp_loop, cvp = cvp_loop)
```


# Create network data


## Nodes 

Systems
Keep one row per system
```{r}
system_nodes = network_master%>%
  select(system_name:num_sw_sources)%>%
  unique()%>%
  rename(id = pwsid)
```

Sources
Keep one row per source. Exclude any PWS sources since these will be in the systems dataset.
```{r}
source_nodes = network_master %>%
  select(source_name:num_systems_served) %>%
  unique()%>%
  rename(id = source_name)
```

Combine

```{r}
all_nodes = full_join(source_nodes, system_nodes, by = "id")%>%
  mutate(type = ifelse(is.na(system_name), "source", "system"))
```



## Edges

Convert dataframe to igraph object (bipartite network)
```{r}
network_edges = network_master%>%
  select(source_name, pwsid)

network = graph_from_data_frame(network_edges, vertices = all_nodes, directed = T)
is_bipartite(network)
```

# Analysis

## Centrality

In, out, and total degree centrality for systems
```{r}
system_centrality = all_nodes%>%
  filter(type == "system")%>%
  rename(in_degree = num_sw_sources,
         out_degree = num_systems_served,
         pwsid = id)%>%
  mutate(total_degree = in_degree + out_degree)%>%
  arrange(-total_degree)%>%
  select(1,4,9,3,11,5:8)

ggplot(system_centrality, aes(total_degree))+
  geom_histogram(fill = "coral2", colour = "black")+
  theme_bw()
```

Out degree centrality for sources
```{r}
source_centrality = all_nodes%>%
  rename(source_name = id, out_degree = num_systems_served)%>%
  filter(type == "source")%>%
  arrange(-out_degree)%>%
  select(1:3)

ggplot(source_centrality, aes(out_degree))+
  geom_histogram(fill = "coral2", colour = "black")+
  theme_bw()
```

# Path length

Calculate shortest paths from sources to systems
```{r}
sources <- V(network)[type == "source"]
systems <- V(network)[type == "system"]

# Get shortest paths from all sources to all systems
path_length_matrix <- distances(network, v = sources, to = systems, mode = "out")
path_lengths = as.vector(path_length_matrix[is.finite(as.vector(path_length_matrix))])
```

Summary stats for sources' path lengths
```{r}
data.frame(
  source_id = sources$name,
  source_type = V(network)[sources]$source_type,
  mean_path_length = apply(path_length_matrix, 1, function(x) mean(x[is.finite(x)])),
  median_path_length = apply(path_length_matrix, 1, function(x) median(x[is.finite(x)])),
  max_path_length = apply(path_length_matrix, 1, function(x) max(x[is.finite(x)])),
  min_path_length = apply(path_length_matrix, 1, function(x) min(x[is.finite(x)])),
  n_reachable_systems = apply(path_length_matrix, 1, function(x) sum(is.finite(x))))%>%
  arrange(-mean_path_length)
```

Summary stats for systems' path lengths

Right now, this is ONLY when systems are the final node on a path
```{r}
data.frame(
  system_id = systems$name,
  system_size = V(network)[systems]$system_size,
  provider_type = V(network)[systems]$provider_type,
  mean_path_length = apply(path_length_matrix, 2, function(x) mean(x[is.finite(x)])),
  median_path_length = apply(path_length_matrix, 2, function(x) median(x[is.finite(x)])),
  max_path_length = apply(path_length_matrix, 2, function(x) max(x[is.finite(x)])),
  min_path_length = apply(path_length_matrix, 2, function(x) min(x[is.finite(x)])),
  n_reachable_sources = apply(path_length_matrix, 2, function(x) sum(is.finite(x))))%>%
  arrange(-mean_path_length)
```


# Bipartite project

Create one-mode projection on systems (systems connected if they share sources)
```{r}
system_adjacency <- bipartite.projection(network)$proj2

# Degree in the projection = number of other systems sharing sources
system_shared_sources_degree <- degree(system_adjacency)
system_degrees$shared_sources_degree <- system_shared_sources_degree
```


```{r}
# VULNERABILITY METRICS
# 1. Source dependency ratio (for systems)
system_degrees$source_dependency_ratio <- ifelse(
  system_degrees$out_degree > 0, 
  system_degrees$in_degree / system_degrees$out_degree, 
  NA
)

# 2. Calculate betweenness centrality - systems that are bottlenecks
betweenness_all <- betweenness(network, directed = TRUE)
system_betweenness <- betweenness_all[V(network)$type == "system"]
system_degrees$betweenness <- system_betweenness

# 3. Calculate eigenvector centrality - systems connected to important systems
eigen_centrality <- eigen_centrality(network, directed = TRUE)$vector
system_degrees$eigen_centrality <- eigen_centrality[V(network)$type == "system"]

# ADDITIONAL DROUGHT-RELATED METRICS
# Incorporate your edge attributes for vulnerability analysis

# If you have edge attributes for purchase arrangements
if("purchased" %in% list.edge.attributes(network)) {
  purchased_edges <- E(network)[purchased == 1]
  
  # Systems heavily reliant on purchased water
  purchased_in_degree <- degree(network, v = V(network)[type == "system"], 
                               mode = "in", loops = FALSE)[V(network)[type == "system"]]
  system_degrees$purchased_water_dependency <- purchased_in_degree / 
    ifelse(in_degree_systems > 0, in_degree_systems, 1)
}

# Source concentration index (Herfindahl-like)
# For each system, how concentrated are their sources?
source_weights <- sapply(V(network)[type == "system"], function(system) {
  sources_to_system <- neighbors(network, system, mode = "in")
  if(length(sources_to_system) == 0) return(NA)
  
  # If you have edge weights (e.g., volume), use them, otherwise equal weights
  if("weight" %in% list.edge.attributes(network)) {
    weights <- E(network)[to(system)]$weight
    sum_weights <- sum(weights)
    sum((weights/sum_weights)^2)  # Herfindahl index
  } else {
    # Simple concentration: 1/number of sources
    1/length(sources_to_system)
  }
})
system_degrees$source_concentration <- source_weights

# MERGE WITH SYSTEM ATTRIBUTES
# Assuming you have vertex attributes for system characteristics
system_attributes <- data.frame(
  system_id = V(network)[type == "system"]$name,
  provider_type = V(network)[type == "system"]$provider_type,
  system_size = V(network)[type == "system"]$system_size,
  population_served = V(network)[type == "system"]$population_served
)

# Combine all metrics
system_vulnerability <- merge(system_degrees, system_attributes, by = "system_id")

# DROUGHT VULNERABILITY SCORING
# Create a composite vulnerability score (example - adjust weights based on your research)
system_vulnerability$vulnerability_score <- 
  (scale(system_vulnerability$source_dependency_ratio) * 0.3) +
  (scale(system_vulnerability$source_concentration) * 0.3) +
  (scale(system_vulnerability$betweenness) * 0.2) +
  (scale(ifelse(is.na(system_vulnerability$purchased_water_dependency), 0, 
                system_vulnerability$purchased_water_dependency)) * 0.2)

# IDENTIFY HIGH-RISK SYSTEMS
high_risk_systems <- system_vulnerability[
  system_vulnerability$vulnerability_score > 
    quantile(system_vulnerability$vulnerability_score, 0.75, na.rm = TRUE),
]

cat("High-risk systems identified:", nrow(high_risk_systems), "\n")

# SUMMARY STATISTICS BY SYSTEM CHARACTERISTICS
library(dplyr)
vulnerability_by_size <- system_vulnerability %>%
  group_by(system_size) %>%
  summarise(
    mean_vulnerability = mean(vulnerability_score, na.rm = TRUE),
    mean_source_dependency = mean(source_dependency_ratio, na.rm = TRUE),
    n_systems = n()
  )

vulnerability_by_provider <- system_vulnerability %>%
  group_by(provider_type) %>%
  summarise(
    mean_vulnerability = mean(vulnerability_score, na.rm = TRUE),
    mean_shared_sources = mean(shared_sources_degree, na.rm = TRUE),
    n_systems = n()
  )

print(vulnerability_by_size)
print(vulnerability_by_provider)
```


